---
title: Module 4 Overview
description: Overview of Module 4 - Vision-Language-Action (VLA)
sidebar_label: Overview
id: overview
experience_level: Advanced
---

# Module 4: Vision-Language-Action (VLA)

Welcome to Module 4! This module is where the worlds of Generative AI and Robotics converge. We will build robots that not only see and move, but also listen and think, enabling natural and intelligent interaction.

## Curriculum

1.  **[The Voice Interface (Ears)](/docs/module-04/01-voice-to-action)**: Capturing and transcribing human commands.
2.  **[Cognitive Planning (The Brain)](/docs/module-04/02-cognitive-planning)**: Using LLMs to translate intent into robot actions.
3.  **[Humanoid Mechanics (The Body)](/docs/module-04/03-humanoid-mechanics)**: Understanding and controlling advanced robot locomotion and manipulation.
4.  **[Capstone: The Autonomous Humanoid](/docs/module-04/04-final-capstone)**: Building a "Butler Bot" capable of understanding and executing complex tasks.

## Goals

*   Integrate speech recognition (OpenAI Whisper) into a ROS 2 robot.
*   Leverage Large Language Models (LLMs) for high-level task planning and command interpretation.
*   Understand the principles of humanoid locomotion and manipulation.
*   Build a complete Vision-Language-Action (VLA) pipeline for an autonomous robot.
