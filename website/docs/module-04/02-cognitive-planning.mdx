---
title: Cognitive Planning (The Brain)
sidebar_label: 2. Cognitive Planning
id: 02-cognitive-planning
description: Using Large Language Models (LLMs) for high-level robot planning and task execution.
custom_edit_url: null
experience_level: Advanced
---

# Cognitive Planning: The Brain

<HeroBox title="From 'Clean the Room' to Robot Actions" variant="green">
  <p>
    Now that our robot can hear, how does it "think"? This chapter explores how Large Language Models (LLMs) act as the robot's high-level brain, translating abstract human commands into concrete sequences of robotic actions.
  </p>
</HeroBox>

## Learning Objectives

By the end of this chapter, you will be able to:

-   Understand the role of LLMs in robotic task planning.
-   Design system prompts to force LLMs to output structured JSON.
-   Grasp the concept of Vision-Language-Action (VLA) architectures.
-   Implement a ROS 2 "Planner Node" that interfaces with an LLM.

## 1. From Text to JSON Actions

LLMs are revolutionizing human-robot interaction. However, they are generally:
*   **Bad at controlling motors directly**: Too slow for real-time control, prone to hallucination of low-level details.
*   **Excellent at planning**: They excel at understanding high-level intent, reasoning, and breaking down complex tasks into logical sub-steps.

### The Translator: LLM to ROS 2 Actions

The key is to use the LLM as a "translator" or "planner." It converts a natural language command into a structured, machine-readable formatâ€”ideally a sequence of ROS 2 Actions.

**Example:** "Clean the room" could be translated by an LLM into:

1.  `Maps_to(location="kitchen")`
2.  `Scan_for(object="trash", location="kitchen")`
3.  `Pick_up(object="trash", target_location="bin")`

### Prompt Engineering for Strict JSON Output

To make an LLM output reliable JSON, you need robust **Prompt Engineering**. This involves designing a "System Prompt" that clearly instructs the LLM on its role, available tools, and the expected output format. OpenAI's API, with its function calling feature, is particularly well-suited for this.

**System Prompt Example (from `context7`: /openai/openai-python)**
```python
# From context7: /openai/openai-python
from pydantic import BaseModel, Field
from typing import List

class RobotMoveAction(BaseModel):
    """Move the robot to a specified location."""
    location: str = Field(..., description="The target location for the robot.")

class RobotGraspAction(BaseModel):
    """Grasp a specified object."""
    object_name: str = Field(..., description="The name of the object to grasp.")

class RobotScanAction(BaseModel):
    """Scan an area for specific objects."""
    area: str = Field(..., description="The area to scan (e.g., 'kitchen', 'table').")
    object_type: str = Field(..., description="The type of object to scan for (e.g., 'trash', 'cup').")

# Define available tools (conceptual - these would be registered with the LLM)
tools = [
    {
        "type": "function",
        "function": {
            "name": "move_robot",
            "description": "Move the robot to a specified location.",
            "parameters": RobotMoveAction.model_json_schema()
        },
    },
    {
        "type": "function",
        "function": {
            "name": "grasp_object",
            "description": "Grasp a specified object.",
            "parameters": RobotGraspAction.model_json_schema()
        },
    },
    {
        "type": "function",
        "function": {
            "name": "scan_area",
            "description": "Scan an area for specific objects.",
            "parameters": RobotScanAction.model_json_schema()
        },
    },
]

SYSTEM_PROMPT = """
You are a helpful robot task planner. Your goal is to convert high-level human commands
into a sequence of robot actions using the available tools.
Only output JSON that calls the provided tools. Do not generate conversational text.
If a command requires multiple steps, output a list of actions.
"""

# Example usage with OpenAI client (conceptual, for demonstration)
# client = OpenAI()
# response = client.chat.completions.create(
#     model="gpt-4o-2024-08-06",
#     messages=[
#         {"role": "system", "content": SYSTEM_PROMPT},
#         {"role": "user", "content": "Clean the room by going to the kitchen, finding all trash, and picking them up."}
#     ],
#     tools=tools,
#     tool_choice="auto" # Let the LLM decide which tool(s) to call
# )

# # Example of parsing LLM's tool call response
# tool_calls = response.choices[0].message.tool_calls
# for tool_call in tool_calls:
#     if tool_call.function.name == "move_robot":
#         parsed_action = RobotMoveAction.model_validate_json(tool_call.function.arguments)
#         print(f"LLM wants to Move to: {parsed_action.location}")
#     elif tool_call.function.name == "scan_area":
#         parsed_action = RobotScanAction.model_validate_json(tool_call.function.arguments)
#         print(f"LLM wants to Scan: {parsed_action.object_type} in {parsed_action.area}")
```

## 2. The "VLA" Architecture

**VLA (Vision-Language-Action)** is an emerging architectural paradigm that unifies a robot's perception (vision), understanding (language), and physical execution (action). It aims to create truly intelligent and adaptable robots.

**Visual: VLA Model Overview**
```mermaid
graph TD;
    A[Microphone/Camera (Sensor Input)] --> B[VLA Model (Whisper, LLM, Vision AI)];
    B --> C[Robot Action (JSON Command)];
    C --> D[ROS 2 Action Server (Robot Execution)];
```

### Code Lab: Build a "Planner Node"

**Objective:** Create a ROS 2 node that subscribes to transcribed human commands (`/human_command`), calls an LLM API to generate a sequence of robot actions in JSON format, and then sends these actions as goals to a ROS 2 Action Server.

**Implementation Overview:**
1.  **Subscribe to `/human_command`**: Listen for text from the "Listening Node."
2.  **Call LLM API**: Use the OpenAI Python client to send the human command to an LLM, along with the system prompt and tool definitions.
3.  **Parse LLM Response**: Extract the JSON actions from the LLM's response.
4.  **Send ROS 2 Action Goals**: For each JSON action, create a corresponding ROS 2 Action Goal and send it to the appropriate Action Server (e.g., a `robot_move` Action Server, a `robot_grasp` Action Server).

**Python Planner Node (Workstation/Cloud)**

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Dict, Any
import json

# Define Pydantic models for robot actions (from Data Model)
class RobotMoveAction(BaseModel):
    location: str

class RobotGraspAction(BaseModel):
    object_name: str

class RobotScanAction(BaseModel):
    area: str
    object_type: str

# Define a unified action model if needed, or handle each tool call separately
class LLMToolCall(BaseModel):
    name: str
    arguments: Dict[str, Any]

class PlannerNode(Node):
    def __init__(self):
        super().__init__('planner_node')
        self.subscription = self.create_subscription(
            String,
            'human_command',
            self.human_command_callback,
            10
        )
        self.openai_client = OpenAI() # Assumes OPENAI_API_KEY is set in environment

        # ROS 2 Action Clients would be initialized here (e.g., for MoveAction, GraspAction)
        # from rclpy.action import ActionClient
        # from robot_interfaces.action import Move, Grasp # Custom Action Messages
        # self._move_action_client = ActionClient(self, Move, 'robot_move')

        self.get_logger().info("Planner Node started. Waiting for human commands.")

    def human_command_callback(self, msg):
        human_command = msg.data
        self.get_logger().info(f"Received human command: '{human_command}'")
        
        # Define the tools available to the LLM (from Data Model)
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "move_robot",
                    "description": "Move the robot to a specified location.",
                    "parameters": RobotMoveAction.model_json_schema()
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "grasp_object",
                    "description": "Grasp a specified object.",
                    "parameters": RobotGraspAction.model_json_schema()
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "scan_area",
                    "description": "Scan an area for specific objects.",
                    "parameters": RobotScanAction.model_json_schema()
                },
            },
        ]

        # System prompt to guide the LLM
        SYSTEM_PROMPT = """
        You are a helpful robot task planner. Your goal is to convert high-level human commands
        into a sequence of robot actions using the available tools.
        Only output JSON that calls the provided tools. Do not generate conversational text.
        If a command requires multiple steps, output a list of actions.
        """

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-2024-08-06", # Use a model that supports tool calling
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": human_command}
                ],
                tools=tools,
                tool_choice="auto"
            )

            tool_calls = response.choices[0].message.tool_calls
            if tool_calls:
                for tool_call in tool_calls:
                    function_name = tool_call.function.name
                    function_args_json = tool_call.function.arguments
                    
                    self.get_logger().info(f"LLM proposed action: {function_name} with args: {function_args_json}")

                    # Here, you would translate LLM's JSON into a ROS 2 Action Goal
                    # and send it via the appropriate Action Client.
                    # Example conceptual:
                    # if function_name == "move_robot":
                    #     parsed_args = RobotMoveAction.model_validate_json(function_args_json)
                    #     move_goal = Move.Goal(location=parsed_args.location)
                    #     self._move_action_client.send_goal_async(move_goal)
                    
                    # For demonstration, we'll just publish the JSON
                    msg_out = String()
                    msg_out.data = json.dumps({"action": function_name, "args": json.loads(function_args_json)})
                    self.publisher_.publish(msg_out) # Assuming a publisher to /robot_goal
                    self.get_logger().info(f"Published robot goal: {msg_out.data}")
            else:
                self.get_logger().warn("LLM did not propose a tool call.")

        except Exception as e:
            self.get_logger().error(f"Error calling LLM API: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = PlannerNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

**Next:** Let's delve into the physical challenges of **Humanoid Mechanics** and how to control complex robot bodies.